{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "35d15c28",
      "metadata": {
        "id": "35d15c28"
      },
      "source": [
        "# Env Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "223c6ad6",
      "metadata": {
        "id": "223c6ad6"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af15b487",
      "metadata": {
        "id": "af15b487",
        "outputId": "fcbe9681-b246-46e5-9ec2-aa2a34fc440e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 17 08:43:01 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7455b23",
      "metadata": {
        "id": "b7455b23"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "322f033f",
      "metadata": {
        "id": "322f033f"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import List, Literal, Union, Dict\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "EnvModeType = Literal[\"colab\", \"remote\", \"local\"]\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Config:\n",
        "    env_mode: EnvModeType\n",
        "    selected_subsets: List[str] = field(\n",
        "        default_factory=lambda: [\"SUDOKU_4\", \"SUDOKU_5\", \"SUDOKU_6\"]\n",
        "    )\n",
        "\n",
        "    # Derived fields (init=False)\n",
        "    data_root: Path = field(init=False)\n",
        "    base_dir: Path = field(init=False)\n",
        "    taco_raw_dir: Path = field(init=False)\n",
        "    taco_file_paths: List[Path] = field(init=False)\n",
        "    normalized_sets_dir: Path = field(init=False)\n",
        "    finetune_dir: Path = field(init=False)\n",
        "    train_dir: Path = field(init=False)\n",
        "    val_dir: Path = field(init=False)\n",
        "    test_dir: Path = field(init=False)\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        # Set data_root based on env_mode (defaults; override in factory if needed)\n",
        "        data_root_map = {\n",
        "            # \"local\": Path.home() / \"mnt/shared\",\n",
        "            \"local\": Path(\"/mnt/shared\"),\n",
        "            # \"remote\": Path(\"/mnt/data\"),\n",
        "            \"remote\": Path.home(),\n",
        "            \"colab\": Path(\"/content/drive/MyDrive\"),\n",
        "        }\n",
        "        object.__setattr__(\n",
        "            self, \"data_root\", data_root_map.get(self.env_mode, Path.cwd())\n",
        "        )\n",
        "\n",
        "        # Derive other paths\n",
        "        object.__setattr__(\n",
        "            self, \"base_dir\", self.data_root / \"datasets/sen2venus\"\n",
        "        )\n",
        "        # object.__setattr__(self, \"taco_raw_dir\", self.base_dir / \"TACO_raw_data\")\n",
        "        # object.__setattr__(\n",
        "        #     self,\n",
        "        #     \"taco_file_paths\",\n",
        "        #     [self.taco_raw_dir / f\"{subset}.taco\" for subset in self.selected_subsets],\n",
        "        # )\n",
        "        object.__setattr__(\n",
        "            self, \"normalized_sets_dir\", self.base_dir / \"normalized_sets\"\n",
        "        )\n",
        "        object.__setattr__(self, \"finetune_dir\", self.base_dir / \"finetune\")\n",
        "        object.__setattr__(self, \"train_dir\", self.normalized_sets_dir / \"train\")\n",
        "        object.__setattr__(self, \"val_dir\", self.normalized_sets_dir / \"val\")\n",
        "        object.__setattr__(self, \"test_dir\", self.normalized_sets_dir / \"test\")\n",
        "\n",
        "    def validate(self) -> None:\n",
        "        \"\"\"Validate config paths exist; raise errors otherwise.\"\"\"\n",
        "        missing_paths = []\n",
        "        for attr in [\n",
        "            \"data_root\",\n",
        "            \"base_dir\",\n",
        "            # \"taco_raw_dir\",\n",
        "            \"normalized_sets_dir\",\n",
        "            \"finetune_dir\",\n",
        "            \"train_dir\",\n",
        "            \"val_dir\",\n",
        "            \"test_dir\",\n",
        "        ]:\n",
        "            path: Path = getattr(self, attr)\n",
        "            if not path.exists():\n",
        "                missing_paths.append(str(path))\n",
        "        if missing_paths:\n",
        "            raise ValueError(f\"Missing paths: {', '.join(missing_paths)}\")\n",
        "        # for file_path in self.taco_file_paths:\n",
        "        #     if not file_path.exists():\n",
        "        #         missing_paths.append(str(file_path))\n",
        "        if missing_paths:\n",
        "            raise ValueError(f\"Missing taco files: {', '.join(missing_paths)}\")\n",
        "\n",
        "\n",
        "def setup_environment(env_mode: EnvModeType) -> None:\n",
        "    \"\"\"Perform environment-specific setup (side effects isolated here).\"\"\"\n",
        "    if env_mode == \"colab\":\n",
        "        try:\n",
        "            import super_image\n",
        "        except ImportError:\n",
        "            print(\"Installing 'super-image'...\")\n",
        "            try:\n",
        "                subprocess.run([\"pip\", \"install\", \"--quiet\", \"super-image\"], check=True)\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                raise RuntimeError(f\"Failed to install super-image: {e}\")\n",
        "\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "\n",
        "            drive.mount(\"/content/drive\", force_remount=True)\n",
        "        except ImportError:\n",
        "            raise RuntimeError(\"Google Colab module not found. Are you in Colab?\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to mount Google Drive: {e}\")\n",
        "\n",
        "        # Optional: Copy data to local /content for faster I/O in Colab\n",
        "        colab_vm_dir = Path(\"/content/taco_normalized\")\n",
        "        if not colab_vm_dir.exists():\n",
        "            print(\"Copying normalized data to local Colab storage for performance...\")\n",
        "            shutil.copytree(\n",
        "                Path(\n",
        "                    \"/content/drive/MyDrive/datasets/sen2venus/normalized_sets\"\n",
        "                ),\n",
        "                colab_vm_dir,\n",
        "            )\n",
        "            print(\"Copy complete.\")\n",
        "        # Avoid os.chdir; let users handle working dir if needed\n",
        "\n",
        "    elif env_mode == \"remote\":\n",
        "        print(\"Remote environment detected. No specific setup needed.\")\n",
        "\n",
        "    elif env_mode == \"local\":\n",
        "        print(\"Local environment detected. Ensuring dependencies...\")\n",
        "\n",
        "\n",
        "def create_config(env_mode: EnvModeType | None = None) -> Config:\n",
        "    \"\"\"Factory to create and setup config based on detected environment.\"\"\"\n",
        "    if env_mode is None:\n",
        "        if \"google.colab\" in sys.modules:\n",
        "            env_mode = \"colab\"\n",
        "        elif \"REMOTE_ENV_VAR\" in os.environ:\n",
        "            env_mode = \"remote\"\n",
        "        else:\n",
        "            env_mode = \"local\"\n",
        "\n",
        "    setup_environment(env_mode)\n",
        "    config = Config(env_mode=env_mode)\n",
        "    config.validate()\n",
        "    return config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003e8698",
      "metadata": {
        "id": "003e8698"
      },
      "outputs": [],
      "source": [
        "config = create_config('colab')\n",
        "print(config.data_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06da0aa8",
      "metadata": {
        "id": "06da0aa8"
      },
      "source": [
        "## Project imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4478ee67",
      "metadata": {
        "id": "4478ee67"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from super_image import Trainer, PreTrainedModel, TrainingArguments\n",
        "from super_image.models import EdsrModel\n",
        "from super_image.trainer import Trainer, logger\n",
        "from super_image.utils.metrics import AverageMeter, compute_metrics\n",
        "from super_image.data import EvalMetrics\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from tqdm.auto import tqdm\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c3d0056",
      "metadata": {
        "id": "7c3d0056"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a96cab0c",
      "metadata": {
        "id": "a96cab0c"
      },
      "outputs": [],
      "source": [
        "class PreNormalizedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Efficiently reads pre-processed, sharded tensor files from disk.\n",
        "    \"\"\"\n",
        "    def __init__(self, shard_dir: Union[str, Path]):\n",
        "        self.shard_dir = Path(shard_dir)\n",
        "        self.shard_paths: List[Path] = sorted(self.shard_dir.glob(\"*.pt\"))\n",
        "\n",
        "        if not self.shard_paths:\n",
        "            raise ValueError(f\"No shard files ('*.pt') found in {self.shard_dir}\")\n",
        "\n",
        "        # To calculate length, we check the size of the first shard and assume\n",
        "        # all but the last are the same size.\n",
        "        first_shard = torch.load(self.shard_paths[0])\n",
        "        self.shard_size = len(first_shard)\n",
        "        last_shard = torch.load(self.shard_paths[-1])\n",
        "        self.length = (len(self.shard_paths) - 1) * self.shard_size + len(last_shard)\n",
        "\n",
        "        # Simple cache to avoid re-loading the same shard consecutively\n",
        "        self._cache = {}\n",
        "        self._cached_shard_index = -1\n",
        "        print(f\"Initialized dataset from {self.shard_dir} with {self.length} samples across {len(self.shard_paths)} shards.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx)->Dict[str, np.ndarray]:\n",
        "        shard_index = idx // self.shard_size\n",
        "        index_in_shard = idx % self.shard_size\n",
        "\n",
        "        if shard_index != self._cached_shard_index:\n",
        "            self._cache = torch.load(self.shard_paths[shard_index])\n",
        "            self._cached_shard_index = shard_index\n",
        "\n",
        "        # coupled with TACORGBDataset dataset class\n",
        "        # each item in the shard is a squeezed dictionary with keys lr and hr\n",
        "        squeezed_sample = self._cache[index_in_shard]\n",
        "        return squeezed_sample['lr'], squeezed_sample['hr']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    test_data_dir = Path(\"/content/taco_normalized/test\") if config.env_mode == \"colab\" else config.test_dir\n",
        "\n",
        "    test_dataset = PreNormalizedDataset(test_data_dir)\n",
        "    # For evaluation, batch size is typically 1 to measure per-image metrics.\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
        "    print(f\"Loaded test dataset with {len(test_dataset)} samples.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: Failed to load the test dataset from '{test_data_dir}': {e}\")"
      ],
      "metadata": {
        "id": "IOwc6rhFKSSI"
      },
      "id": "IOwc6rhFKSSI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ffb66367",
      "metadata": {
        "id": "ffb66367"
      },
      "source": [
        "# Load the Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(finetune_dir: Path|str, pre_trained : bool, scale: int = 2):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    test_data_dir = config.test_dir\n",
        "    checkpoint_path = finetune_dir / 'best_model_checkpoint.pt'\n",
        "\n",
        "    if pre_trained:\n",
        "        model = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=2)\n",
        "    else:\n",
        "        model = EdsrModel(scale=2)\n",
        "\n",
        "    # If multiple GPUs are available, wrap the model in DataParallel\n",
        "    # This ensures the model's structure matches the training environment.\n",
        "    if torch.cuda.device_count() > 1:\n",
        "      print(f\"Using {torch.cuda.device_count()} GPUs. Wrapping model in DataParallel.\")\n",
        "      model = torch.nn.DataParallel(model)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # load the state dictionary from best checkpoint\n",
        "    print(f\"Loading best model checkpoint from: {checkpoint_path}\")\n",
        "    try:\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model_state_dict = checkpoint['model_state_dict']\n",
        "\n",
        "        # --- Logic to handle all DataParallel cases ---\n",
        "        is_model_parallel = isinstance(model, torch.nn.DataParallel)\n",
        "        is_checkpoint_parallel = list(model_state_dict.keys())[0].startswith('module.')\n",
        "\n",
        "        final_state_dict = OrderedDict()\n",
        "\n",
        "        if is_model_parallel and not is_checkpoint_parallel:\n",
        "            # If the current model is parallel but the checkpoint isn't, add \"module.\" prefix\n",
        "            print(\"Model is parallel, checkpoint is not. Adding 'module.' prefix to keys...\")\n",
        "            for k, v in model_state_dict.items():\n",
        "                final_state_dict['module.' + k] = v\n",
        "        elif not is_model_parallel and is_checkpoint_parallel:\n",
        "            # If the checkpoint is parallel but the current model isn't, strip \"module.\" prefix\n",
        "            print(\"Checkpoint is parallel, model is not. Stripping 'module.' prefix from keys...\")\n",
        "            for k, v in model_state_dict.items():\n",
        "                final_state_dict[k[7:]] = v # k[7:] removes 'module.'\n",
        "        else:\n",
        "            # If they are both parallel or both not parallel, the keys match already.\n",
        "            print(\"Model and checkpoint states match. Loading directly.\")\n",
        "            final_state_dict = model_state_dict\n",
        "\n",
        "        # Load the correctly formatted state dictionary\n",
        "        model.load_state_dict(final_state_dict)\n",
        "\n",
        "        print(f\"\\n✅Successfully loaded model from epoch {checkpoint['epoch']} with best validation PSNR {checkpoint['best_metric']:.4f}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ ERROR: Checkpoint file not found at '{checkpoint_path}'. Please verify the path.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR: An error occurred while loading the checkpoint: {e}\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    return model, device"
      ],
      "metadata": {
        "id": "h_3FDUAwyo8H"
      },
      "id": "h_3FDUAwyo8H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_16_block, device_16_block = load_model(config.finetune_dir / 'edsr_base', pre_trained=True)"
      ],
      "metadata": {
        "id": "tvSB3L9MfjeD"
      },
      "id": "tvSB3L9MfjeD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_8_block, device_8_block = load_model(config.finetune_dir / 'edsr_base_8_block', pre_trained=False)"
      ],
      "metadata": {
        "id": "akjcwjwd1hdh"
      },
      "id": "akjcwjwd1hdh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0151e3b5",
      "metadata": {
        "id": "0151e3b5"
      },
      "source": [
        "# Step 1: Quantitative Evaluation on the Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trained 8 block"
      ],
      "metadata": {
        "id": "WllmNufr1emu"
      },
      "id": "WllmNufr1emu"
    },
    {
      "cell_type": "code",
      "source": [
        "EvalMetrics().evaluate(model_16_block, test_dataset)\n",
        "print(f\"Total images evaluated: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "CaRd8TPGJqSx"
      },
      "id": "CaRd8TPGJqSx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EvalMetrics().evaluate(model_8_block, test_dataset)\n",
        "print(f\"Total images evaluated: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "D92tGtX6Jnva"
      },
      "id": "D92tGtX6Jnva",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Qualitative Visual Analysis"
      ],
      "metadata": {
        "id": "0W3hqk9uKa6R"
      },
      "id": "0W3hqk9uKa6R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## convert tensors to displayable images"
      ],
      "metadata": {
        "id": "6_kYlLsqK_Et"
      },
      "id": "6_kYlLsqK_Et"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3: Helper function to convert tensors to displayable images ---\n",
        "def tensor_to_image(tensor):\n",
        "    \"\"\"Converts a PyTorch tensor to a NumPy image for plotting.\"\"\"\n",
        "    # Move tensor to CPU and convert to NumPy array\n",
        "    image = tensor.cpu().numpy()\n",
        "    # Tranpose from (C, H, W) to (H, W, C)\n",
        "    image = np.transpose(image, (1, 2, 0))\n",
        "    # Clip values to be in the valid [0, 1] range for display\n",
        "    image = np.clip(image, 0, 1)\n",
        "    return image\n",
        "\n",
        "# --- Step 4: The Main Plotting Function ---\n",
        "def plot_comparison(dataset, model, index, zoom_rect, device):\n",
        "    \"\"\"\n",
        "    Selects an image by index, runs inference, and plots a detailed\n",
        "    side-by-side comparison with a zoomed-in patch.\n",
        "\n",
        "    Args:\n",
        "        dataset: The dataset to draw an image from.\n",
        "        model: The trained model to use for inference.\n",
        "        index: The index of the image in the dataset.\n",
        "        zoom_rect: A tuple (x, y, width, height) defining the zoom area.\n",
        "    \"\"\"\n",
        "    # Get the LR and HR images from the dataset\n",
        "    lr_tensor, hr_tensor = dataset[index]\n",
        "\n",
        "    # Run inference to get the Super-Resolved image\n",
        "    with torch.no_grad():\n",
        "        # Add a batch dimension and send to device\n",
        "        lr_batch = lr_tensor.unsqueeze(0).to(device)\n",
        "        sr_tensor = model(lr_batch)\n",
        "        # Remove the batch dimension\n",
        "        sr_tensor = sr_tensor.squeeze(0)\n",
        "\n",
        "    # Convert all tensors to NumPy images for plotting\n",
        "    lr_image = tensor_to_image(lr_tensor)\n",
        "    sr_image = tensor_to_image(sr_tensor)\n",
        "    hr_image = tensor_to_image(hr_tensor)\n",
        "\n",
        "    # --- Plotting ---\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(20, 14))\n",
        "\n",
        "    # Plot the full images\n",
        "    axes[0, 0].imshow(lr_image)\n",
        "    axes[0, 0].set_title('Low-Resolution Input', fontsize=16)\n",
        "\n",
        "    axes[0, 1].imshow(sr_image)\n",
        "    axes[0, 1].set_title('Super-Resolved Output', fontsize=16)\n",
        "\n",
        "    axes[0, 2].imshow(hr_image)\n",
        "    axes[0, 2].set_title('Ground Truth High-Resolution', fontsize=16)\n",
        "\n",
        "    # Add rectangles to show the zoom area on the full images\n",
        "    x, y, w, h = zoom_rect\n",
        "    for ax in axes[0, :]:\n",
        "        rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Plot the zoomed-in patches\n",
        "    axes[1, 0].imshow(lr_image[y:y+h, x:x+w, :])\n",
        "    axes[1, 0].set_title('Zoomed LR', fontsize=16)\n",
        "\n",
        "    axes[1, 1].imshow(sr_image[y:y+h, x:x+w, :])\n",
        "    axes[1, 1].set_title('Zoomed SR', fontsize=16)\n",
        "\n",
        "    axes[1, 2].imshow(hr_image[y:y+h, x:x+w, :])\n",
        "    axes[1, 2].set_title('Zoomed HR', fontsize=16)\n",
        "\n",
        "    for ax in axes[1, :]:\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7juoBDvdK7Mj"
      },
      "id": "7juoBDvdK7Mj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate the Comparisons"
      ],
      "metadata": {
        "id": "ZZpYnpfbLS3b"
      },
      "id": "ZZpYnpfbLS3b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: A general area\n",
        "plot_comparison(test_dataset, model_16_block, index=50, zoom_rect=(40, 50, 48, 48), device=device_16_block)\n",
        "\n",
        "# Example 2: Try another image index\n",
        "plot_comparison(test_dataset, model_16_block, index=120, zoom_rect=(60, 20, 48, 48), device=device_16_block)\n",
        "\n",
        "# Example 3: Find an image with a distinct feature, like a road or building\n",
        "plot_comparison(test_dataset, model_16_block, index=250, zoom_rect=(30, 70, 48, 48), device=device_16_block)"
      ],
      "metadata": {
        "id": "T5CcloXoLNGU"
      },
      "id": "T5CcloXoLNGU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: A general area\n",
        "plot_comparison(test_dataset, model_8_block, index=50, zoom_rect=(40, 50, 48, 48), device=device_8_block)\n",
        "\n",
        "# Example 2: Try another image index\n",
        "plot_comparison(test_dataset, model_8_block, index=120, zoom_rect=(60, 20, 48, 48), device=device_8_block)\n",
        "\n",
        "# Example 3: Find an image with a distinct feature, like a road or building\n",
        "plot_comparison(test_dataset, model_8_block, index=250, zoom_rect=(30, 70, 48, 48), device=device_8_block)"
      ],
      "metadata": {
        "id": "1mFYEXUDLcI-"
      },
      "id": "1mFYEXUDLcI-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}