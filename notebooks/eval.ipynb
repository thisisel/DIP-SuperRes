{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d15c28",
   "metadata": {},
   "source": [
    "# Env Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af15b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7455b23",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Union, Dict\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "EnvModeType = Literal[\"colab\", \"remote\", \"local\"]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    env_mode: EnvModeType\n",
    "    selected_subsets: List[str] = field(\n",
    "        default_factory=lambda: [\"SUDOKU_4\", \"SUDOKU_5\", \"SUDOKU_6\"]\n",
    "    )\n",
    "\n",
    "    # Derived fields (init=False)\n",
    "    data_root: Path = field(init=False)\n",
    "    base_dir: Path = field(init=False)\n",
    "    taco_raw_dir: Path = field(init=False)\n",
    "    taco_file_paths: List[Path] = field(init=False)\n",
    "    normalized_sets_dir: Path = field(init=False)\n",
    "    finetune_dir: Path = field(init=False)\n",
    "    train_dir: Path = field(init=False)\n",
    "    val_dir: Path = field(init=False)\n",
    "    test_dir: Path = field(init=False)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        # Set data_root based on env_mode (defaults; override in factory if needed)\n",
    "        data_root_map = {\n",
    "            # \"local\": Path.home() / \"mnt/shared\",\n",
    "            \"local\": Path(\"/mnt/shared\"),\n",
    "            # \"remote\": Path(\"/mnt/data\"),\n",
    "            \"remote\": Path.home(),\n",
    "            \"colab\": Path(\"/content/drive/MyDrive\"),\n",
    "        }\n",
    "        object.__setattr__(\n",
    "            self, \"data_root\", data_root_map.get(self.env_mode, Path.cwd())\n",
    "        )\n",
    "\n",
    "        # Derive other paths\n",
    "        object.__setattr__(\n",
    "            self, \"base_dir\", self.data_root / \"datasets/sen2venus\"\n",
    "        )  \n",
    "        # object.__setattr__(self, \"taco_raw_dir\", self.base_dir / \"TACO_raw_data\")\n",
    "        # object.__setattr__(\n",
    "        #     self,\n",
    "        #     \"taco_file_paths\",\n",
    "        #     [self.taco_raw_dir / f\"{subset}.taco\" for subset in self.selected_subsets],\n",
    "        # )\n",
    "        object.__setattr__(\n",
    "            self, \"normalized_sets_dir\", self.base_dir / \"normalized_sets\"\n",
    "        )\n",
    "        object.__setattr__(self, \"finetune_dir\", self.base_dir / \"finetune\")\n",
    "        object.__setattr__(self, \"train_dir\", self.normalized_sets_dir / \"train\")\n",
    "        object.__setattr__(self, \"val_dir\", self.normalized_sets_dir / \"val\")\n",
    "        object.__setattr__(self, \"test_dir\", self.normalized_sets_dir / \"test\")\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate config paths exist; raise errors otherwise.\"\"\"\n",
    "        missing_paths = []\n",
    "        for attr in [\n",
    "            \"data_root\",\n",
    "            \"base_dir\",\n",
    "            # \"taco_raw_dir\",\n",
    "            \"normalized_sets_dir\",\n",
    "            \"finetune_dir\",\n",
    "            \"train_dir\",\n",
    "            \"val_dir\",\n",
    "            \"test_dir\",\n",
    "        ]:\n",
    "            path: Path = getattr(self, attr)\n",
    "            if not path.exists():\n",
    "                missing_paths.append(str(path))\n",
    "        if missing_paths:\n",
    "            raise ValueError(f\"Missing paths: {', '.join(missing_paths)}\")\n",
    "        # for file_path in self.taco_file_paths:\n",
    "        #     if not file_path.exists():\n",
    "        #         missing_paths.append(str(file_path))\n",
    "        if missing_paths:\n",
    "            raise ValueError(f\"Missing taco files: {', '.join(missing_paths)}\")\n",
    "\n",
    "\n",
    "def setup_environment(env_mode: EnvModeType) -> None:\n",
    "    \"\"\"Perform environment-specific setup (side effects isolated here).\"\"\"\n",
    "    if env_mode == \"colab\":\n",
    "        try:\n",
    "            import super_image\n",
    "        except ImportError:\n",
    "            print(\"Installing 'super-image'...\")\n",
    "            try:\n",
    "                subprocess.run([\"pip\", \"install\", \"--quiet\", \"super-image\"], check=True)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                raise RuntimeError(f\"Failed to install super-image: {e}\")\n",
    "\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "\n",
    "            drive.mount(\"/content/drive\", force_remount=True)\n",
    "        except ImportError:\n",
    "            raise RuntimeError(\"Google Colab module not found. Are you in Colab?\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to mount Google Drive: {e}\")\n",
    "\n",
    "        # Optional: Copy data to local /content for faster I/O in Colab\n",
    "        colab_vm_dir = Path(\"/content/taco_normalized\")\n",
    "        if not colab_vm_dir.exists():\n",
    "            print(\"Copying normalized data to local Colab storage for performance...\")\n",
    "            shutil.copytree(\n",
    "                Path(\n",
    "                    \"/content/drive/MyDrive/datasets/sen2venus/normalized_sets\"\n",
    "                ),\n",
    "                colab_vm_dir,\n",
    "            )\n",
    "            print(\"Copy complete.\")\n",
    "        # Avoid os.chdir; let users handle working dir if needed\n",
    "\n",
    "    elif env_mode == \"remote\":\n",
    "        print(\"Remote environment detected. No specific setup needed.\")\n",
    "\n",
    "    elif env_mode == \"local\":\n",
    "        print(\"Local environment detected. Ensuring dependencies...\")\n",
    "\n",
    "\n",
    "def create_config(env_mode: EnvModeType | None = None) -> Config:\n",
    "    \"\"\"Factory to create and setup config based on detected environment.\"\"\"\n",
    "    if env_mode is None:\n",
    "        if \"google.colab\" in sys.modules:\n",
    "            env_mode = \"colab\"\n",
    "        elif \"REMOTE_ENV_VAR\" in os.environ:  # Example detection for remote\n",
    "            env_mode = \"remote\"\n",
    "        else:\n",
    "            env_mode = \"local\"\n",
    "\n",
    "    setup_environment(env_mode)\n",
    "    config = Config(env_mode=env_mode)\n",
    "    config.validate()  # Ensure everything is ready\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e8698",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = create_config()  \n",
    "print(config.data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da0aa8",
   "metadata": {},
   "source": [
    "## Project imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4478ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from super_image import Trainer, PreTrainedModel, TrainingArguments\n",
    "from super_image.models import EdsrModel\n",
    "from super_image.trainer import Trainer, logger\n",
    "from super_image.utils.metrics import AverageMeter, compute_metrics\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3d0056",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96cab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNormalizedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Efficiently reads pre-processed, sharded tensor files from disk.\n",
    "    \"\"\"\n",
    "    def __init__(self, shard_dir: Union[str, Path]):\n",
    "        self.shard_dir = Path(shard_dir)\n",
    "        self.shard_paths: List[Path] = sorted(self.shard_dir.glob(\"*.pt\"))\n",
    "\n",
    "        if not self.shard_paths:\n",
    "            raise ValueError(f\"No shard files ('*.pt') found in {self.shard_dir}\")\n",
    "\n",
    "        # To calculate length, we check the size of the first shard and assume\n",
    "        # all but the last are the same size.\n",
    "        first_shard = torch.load(self.shard_paths[0])\n",
    "        self.shard_size = len(first_shard)\n",
    "        last_shard = torch.load(self.shard_paths[-1])\n",
    "        self.length = (len(self.shard_paths) - 1) * self.shard_size + len(last_shard)\n",
    "\n",
    "        # Simple cache to avoid re-loading the same shard consecutively\n",
    "        self._cache = {}\n",
    "        self._cached_shard_index = -1\n",
    "        print(f\"Initialized dataset from {self.shard_dir} with {self.length} samples across {len(self.shard_paths)} shards.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx)->Dict[str, np.ndarray]:\n",
    "        shard_index = idx // self.shard_size\n",
    "        index_in_shard = idx % self.shard_size\n",
    "\n",
    "        if shard_index != self._cached_shard_index:\n",
    "            self._cache = torch.load(self.shard_paths[shard_index])\n",
    "            self._cached_shard_index = shard_index\n",
    "\n",
    "        # coupled with TACORGBDataset dataset class\n",
    "        # each item in the shard is a squeezed dictionary with keys lr and hr\n",
    "        squeezed_sample = self._cache[index_in_shard]\n",
    "        return squeezed_sample['lr'], squeezed_sample['hr']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0151e3b5",
   "metadata": {},
   "source": [
    "# Step 1: Quantitative Evaluation on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb66367",
   "metadata": {},
   "source": [
    "### 1.1 Load the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb826e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_data_dir = config.test_dir\n",
    "checkpoint_path = config.finetune_dir / 'edsr_base/best_model_checkpoint.pt'\n",
    "\n",
    "# --- Step 3: Load the Fine-Tuned Model ---\n",
    "# First, instantiate the base architecture\n",
    "model = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=2)\n",
    "\n",
    "# Now, load the state dictionary from our best checkpoint\n",
    "print(f\"Loading best model checkpoint from: {checkpoint_path}\")\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # This logic handles cases where the model was saved with DataParallel wrapper\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "    if list(model_state_dict.keys())[0].startswith('module.'):\n",
    "        print(\"DataParallel wrapper detected. Unwrapping state dictionary...\")\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in model_state_dict.items():\n",
    "            name = k[7:] # remove `module.` prefix\n",
    "            new_state_dict[name] = v\n",
    "        model.load_state_dict(new_state_dict)\n",
    "    else:\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        \n",
    "    print(f\"Successfully loaded model from epoch {checkpoint['epoch']} with best validation PSNR {checkpoint['best_metric']:.4f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Checkpoint file not found at '{checkpoint_path}'. Please verify the path.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: An error occurred while loading the checkpoint: {e}\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval() # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63ac97",
   "metadata": {},
   "source": [
    "### 1.2 Prepare the Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87beebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_dataset = PreNormalizedDataset(test_data_dir)\n",
    "    # For evaluation, batch size is typically 1 to measure per-image metrics.\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1) \n",
    "    print(f\"Loaded test dataset with {len(test_dataset)} samples.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: Failed to load the test dataset from '{test_data_dir}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c7d192",
   "metadata": {},
   "source": [
    "### 1.3 Run Evaluation Loop and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b73501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize meters to accumulate the scores\n",
    "test_psnr = AverageMeter()\n",
    "test_ssim = AverageMeter()\n",
    "\n",
    "# Set the scale from the model's configuration\n",
    "scale = model.config.scale\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculations for efficiency\n",
    "    for data in tqdm(test_dataloader, desc=\"Evaluating on Test Set\"):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        preds = model(inputs)\n",
    "        \n",
    "        # The compute_metrics function expects a dictionary\n",
    "        metrics = compute_metrics({'predictions': preds, 'labels': labels}, scale=scale)\n",
    "        \n",
    "        test_psnr.update(metrics['psnr'], len(inputs))\n",
    "        test_ssim.update(metrics['ssim'], len(inputs))\n",
    "\n",
    "# --- Step 6: Report Final Scores ---\n",
    "print(\"\\n--- Final Test Set Evaluation Complete ---\")\n",
    "print(f\"Total images evaluated: {len(test_dataset)}\")\n",
    "print(f\"Final Test Set PSNR: {test_psnr.avg:.4f}\")\n",
    "print(f\"Final Test Set SSIM: {test_ssim.avg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
