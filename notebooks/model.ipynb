{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f72ac18adff47ff925a4f2850dc36fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa9d44503708453b9482591030350704",
              "IPY_MODEL_04449d91732f416dab9a9d0f2f89f5d5",
              "IPY_MODEL_950106b1dcc148dc9586bbb1000f99be"
            ],
            "layout": "IPY_MODEL_ebe0c8a19ad945679b7faa1702108d96"
          }
        },
        "fa9d44503708453b9482591030350704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60362261071f4b099396f41371389e65",
            "placeholder": "​",
            "style": "IPY_MODEL_d563d0f4dd084432adfae11c7752ff57",
            "value": "epoch: 0/14:  28%"
          }
        },
        "04449d91732f416dab9a9d0f2f89f5d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29eb2105ed754070ad2b1621b3430fb1",
            "max": 4416,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_839c28a44918423392a4d0000f526d97",
            "value": 1224
          }
        },
        "950106b1dcc148dc9586bbb1000f99be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f4e0421aa984f91a0d841ddc1451418",
            "placeholder": "​",
            "style": "IPY_MODEL_5dd688f11e194fcdb9180aa6a3b6109c",
            "value": " 1224/4416 [36:44&lt;1:24:53,  1.60s/it, loss=0.005396]"
          }
        },
        "ebe0c8a19ad945679b7faa1702108d96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60362261071f4b099396f41371389e65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d563d0f4dd084432adfae11c7752ff57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29eb2105ed754070ad2b1621b3430fb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "839c28a44918423392a4d0000f526d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f4e0421aa984f91a0d841ddc1451418": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd688f11e194fcdb9180aa6a3b6109c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the Enviorment\n"
      ],
      "metadata": {
        "id": "_KAzAvjKhwb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check out Colab Instance Region"
      ],
      "metadata": {
        "id": "CzeI3F9vjb40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipinfo.io"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7YxlFf-jXTb",
        "outputId": "5974add6-9732-494e-e4e2-f3f98a11b60e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"ip\": \"34.125.179.250\",\n",
            "  \"hostname\": \"250.179.125.34.bc.googleusercontent.com\",\n",
            "  \"city\": \"Las Vegas\",\n",
            "  \"region\": \"Nevada\",\n",
            "  \"country\": \"US\",\n",
            "  \"loc\": \"36.1750,-115.1372\",\n",
            "  \"org\": \"AS396982 Google LLC\",\n",
            "  \"postal\": \"89111\",\n",
            "  \"timezone\": \"America/Los_Angeles\",\n",
            "  \"readme\": \"https://ipinfo.io/missingauth\"\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPNJVYlvvEd2",
        "outputId": "1523da08-7681-41e3-f612-ffa88e4a2b0c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug  9 18:48:20 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0             30W /   70W |    4382MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intsall Essential Packaeges\n",
        "\n",
        "`super-image` library is built on top of **Hugging Face**'s `transformers` and `datasets`"
      ],
      "metadata": {
        "id": "53oNfTDU4aOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install super-image -q"
      ],
      "metadata": {
        "id": "zFprZGbb4ak8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "XNClEaWBiATD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam, lr_scheduler\n",
        "\n",
        "\n",
        "from super_image import Trainer, PreTrainedModel, TrainingArguments\n",
        "from super_image.models import EdsrModel\n",
        "from super_image.trainer import Trainer, logger\n",
        "from super_image.utils.metrics import AverageMeter"
      ],
      "metadata": {
        "id": "qpjGAr7whwyC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Union, Dict\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4u4KLpJbuw7x"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paths and Directories"
      ],
      "metadata": {
        "id": "y80KYwyIiJdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFoBo8t5iKS1",
        "outputId": "6d45a537-aaf2-489a-bb64-3a3a6dcaf878"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = Path('/content/drive/MyDrive/datasets/sen2venus')\n",
        "\n",
        "TACO_RAW_DIR = BASE_DIR / 'TACO_raw_data'\n",
        "os.makedirs(TACO_RAW_DIR, exist_ok=True)\n",
        "print(f\"Data will be saved to: {TACO_RAW_DIR}\")\n",
        "\n",
        "SELECTED_SUBSETS = [\n",
        "    \"SUDOUE-4\",\n",
        "    \"SUDOUE-5\",\n",
        "    \"SUDOUE-6\"\n",
        "]\n",
        "TACO_FILE_PATHS = [TACO_RAW_DIR / f\"{site_name}.taco\" for site_name in SELECTED_SUBSETS]\n",
        "\n",
        "\n",
        "NORMALIZED_SETS_DIR = BASE_DIR / 'normalized_sets'\n",
        "os.makedirs(NORMALIZED_SETS_DIR, exist_ok=True)\n",
        "print(f\"Normalaized sets are retrieved from:\\n\\t {NORMALIZED_SETS_DIR}\")\n",
        "\n",
        "# TRAIN_SAVE_DIR = NORMALIZED_SETS_DIR / 'train'\n",
        "# os.makedirs(TRAIN_SAVE_DIR, exist_ok=True)\n",
        "# print(f\"Train data will be saved to:\\n\\t {TRAIN_SAVE_DIR}\")\n",
        "\n",
        "# VAL_SAVE_DIR = NORMALIZED_SETS_DIR / 'val'\n",
        "# os.makedirs(VAL_SAVE_DIR, exist_ok=True)\n",
        "# print(f\"Validation data will be saved to:\\n\\t {VAL_SAVE_DIR}\")\n",
        "\n",
        "# TEST_SAVE_DIR = NORMALIZED_SETS_DIR / 'test'\n",
        "# os.makedirs(TEST_SAVE_DIR, exist_ok=True)\n",
        "# print(f\"Test data will be saved to:\\n\\t {TEST_SAVE_DIR}\")\n",
        "\n",
        "# essential for resuming training and saving final model.\n",
        "FINETUNR_SAVE_DIR = BASE_DIR / 'edsr_finetune'\n",
        "os.makedirs(FINETUNR_SAVE_DIR, exist_ok=True)\n",
        "print(f\"Finetuning data including checkpoints and logs will be saved to:\\n\\t{FINETUNR_SAVE_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-EoaWOvpdsx",
        "outputId": "a1851bfd-3171-4978-d5a7-9f5a59b45cc4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data will be saved to: /content/drive/MyDrive/datasets/sen2venus/TACO_raw_data\n",
            "Normalaized sets are retrieved from:\n",
            "\t /content/drive/MyDrive/datasets/sen2venus/normalized_sets\n",
            "Finetuning data including checkpoints and logs will be saved to:\n",
            "\t/content/drive/MyDrive/datasets/sen2venus/edsr_finetune\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VM_DIR = '/content/TACO_Normalized'\n",
        "\n",
        "print(\"--- Starting Data Transfer ---\")\n",
        "# Create the local directory if it doesn't exist\n",
        "if not os.path.exists(VM_DIR):\n",
        "    print(f\"Copying data from {NORMALIZED_SETS_DIR} to {VM_DIR}...\")\n",
        "    # Use the -q flag for a quiet copy to avoid flooding your output\n",
        "    !cp -r \"{NORMALIZED_SETS_DIR}\" \"{VM_DIR}\"\n",
        "    print(\"Data transfer complete.\")\n",
        "else:\n",
        "    print(\"Data already exists locally.\")\n",
        "\n",
        "\n",
        "VM_DIR = Path(VM_DIR)\n",
        "TRAIN_VM_DIR = VM_DIR / 'train'\n",
        "VAL_VM_DIR = VM_DIR / 'val'\n",
        "TEST_VM_DIR = VM_DIR / 'test'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi2HE0q8xOr6",
        "outputId": "75f16479-c31a-4006-c2ae-effe25e0aee8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Data Transfer ---\n",
            "Copying data from /content/drive/MyDrive/datasets/sen2venus/normalized_sets to /content/TACO_Normalized...\n",
            "Data transfer complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Define PyTorch Datasets & Dataloaders"
      ],
      "metadata": {
        "id": "ClVyKPm_peAg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_vV2B_ZtT6oW"
      },
      "outputs": [],
      "source": [
        "class PreNormalizedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Efficiently reads pre-processed, sharded tensor files from disk.\n",
        "    \"\"\"\n",
        "    def __init__(self, shard_dir: Union[str, Path]):\n",
        "        self.shard_dir = Path(shard_dir)\n",
        "        self.shard_paths: List[Path] = sorted(self.shard_dir.glob(\"*.pt\"))\n",
        "\n",
        "        if not self.shard_paths:\n",
        "            raise ValueError(f\"No shard files ('*.pt') found in {self.shard_dir}\")\n",
        "\n",
        "        # To calculate length, we check the size of the first shard and assume\n",
        "        # all but the last are the same size.\n",
        "        first_shard = torch.load(self.shard_paths[0])\n",
        "        self.shard_size = len(first_shard)\n",
        "        last_shard = torch.load(self.shard_paths[-1])\n",
        "        self.length = (len(self.shard_paths) - 1) * self.shard_size + len(last_shard)\n",
        "\n",
        "        # Simple cache to avoid re-loading the same shard consecutively\n",
        "        self._cache = {}\n",
        "        self._cached_shard_index = -1\n",
        "        print(f\"Initialized dataset from {self.shard_dir} with {self.length} samples across {len(self.shard_paths)} shards.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx)->Dict[str, np.ndarray]:\n",
        "        shard_index = idx // self.shard_size\n",
        "        index_in_shard = idx % self.shard_size\n",
        "\n",
        "        if shard_index != self._cached_shard_index:\n",
        "            self._cache = torch.load(self.shard_paths[shard_index])\n",
        "            self._cached_shard_index = shard_index\n",
        "\n",
        "        # coupled with TACORGBDataset dataset class\n",
        "        # each item in the shard is a squeezed dictionary with keys lr and hr\n",
        "        squeezed_sample = self._cache[index_in_shard]\n",
        "        return squeezed_sample['lr'], squeezed_sample['hr']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader Instantiation"
      ],
      "metadata": {
        "id": "Iw7l7AuU5IUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PreNormalizedDataset(TRAIN_VM_DIR)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TINjQ7ZN5MZf",
        "outputId": "2281d05e-77ca-4248-8898-04fd30cd64b8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized dataset from /content/TACO_Normalized/train with 4436 samples across 5 shards.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Verifying  batch shape:\")\n",
        "\n",
        "lr_batch, hr_batch = next(iter(train_loader))\n",
        "\n",
        "print(f\"Verification successful!\")\n",
        "print(f\"LR batch shape: {lr_batch.shape}\")\n",
        "print(f\"HR batch shape: {hr_batch.shape}\")\n",
        "print(f\"LR batch dtype: {lr_batch.dtype}\")\n",
        "print(f\"HR batch dtype: {hr_batch.dtype}\")"
      ],
      "metadata": {
        "id": "PI4v6MuK5mgG",
        "outputId": "db98c11b-e600-4f83-af17-060afa7af788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Verifying  dataset output format:\n",
            "dict_keys(['pixel_values', 'labels'])\n",
            "LR shape: torch.Size([3, 128, 128])\n",
            "HR shape: torch.Size([3, 256, 256])\n",
            "---Verifying  batch shape:\n",
            "Verification successful!\n",
            "LR batch shape: torch.Size([16, 3, 128, 128])\n",
            "HR batch shape: torch.Size([16, 3, 256, 256])\n",
            "LR batch dtype: torch.float32\n",
            "HR batch dtype: torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = PreNormalizedDataset(VAL_VM_DIR)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "2-xBNDU35Tcg",
        "outputId": "363d693c-01ed-4f33-8bcd-97063a3b6111",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized dataset from /content/TACO_Normalized/val with 554 samples across 1 shards.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = PreNormalizedDataset(TEST_SAVE_DIR)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfwFUqvdvWSQ",
        "outputId": "114f2261-37e8-4d98-d5b6-a8d17033fa29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized dataset from /content/drive/MyDrive/datasets/sen2venus/normalized_sets/test with 556 samples across 1 shards.\n",
            "Loaded 556 test samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Load the Pre-trained EDSR Model"
      ],
      "metadata": {
        "id": "YUwwsMCp7JIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objectives**:\n",
        "\n",
        "\n",
        "\n",
        "1.   Loading a well-known, **pre-trained** architecture (edsr-base) specifically configured for **2x super-resolution**.\n",
        "2.   Confirming that the model accepts data batches and produces outputs of the correct shape ([16, 3, 256, 256])."
      ],
      "metadata": {
        "id": "B92rUzqB_F-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Instantiate and Inspect the pre-trained EDSR model"
      ],
      "metadata": {
        "id": "DSU_eKfZ8RUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'from_pretrained' method downloads the model configuration and weights.\n",
        "# We must specify our desired scale factor.\n",
        "# (LR: 128x128, HR: 256x256), -> scale is 2.\n",
        "scale = 2\n",
        "model_id = 'eugenesiow/edsr-base'\n",
        "model = EdsrModel.from_pretrained(model_id, scale=scale)\n",
        "\n",
        "# Inspect the model architecture\n",
        "print(\"Model architecture loaded successfully:\")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d8c5mkF7jh0",
        "outputId": "402b09e3-bae2-4495-fc9c-2a7e9cee6861"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://huggingface.co/eugenesiow/edsr-base/resolve/main/pytorch_model_2x.pt\n",
            "Model architecture loaded successfully:\n",
            "DataParallel(\n",
            "  (module): EdsrModel(\n",
            "    (sub_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (add_mean): MeanShift(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (head): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (body): Sequential(\n",
            "      (0): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (1): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (2): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (3): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (4): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (5): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (6): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (7): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (8): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (9): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (10): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (11): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (12): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (13): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (14): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (15): ResBlock(\n",
            "        (body): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (tail): Sequential(\n",
            "      (0): Upsampler(\n",
            "        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): PixelShuffle(upscale_factor=2)\n",
            "      )\n",
            "      (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Sanity Check: Pass one batch of data through the model"
      ],
      "metadata": {
        "id": "z-QbcXpS8ae2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a crucial test to ensure the input/output dimensions are compatible.\n",
        "print(\"\\nPerforming a forward pass sanity check...\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()  # Set to evaluation mode for this check\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Get a single batch from our dataloader\n",
        "    lr_batch, hr_batch = next(iter(train_loader))\n",
        "\n",
        "    # Move the batch to the same device as the model\n",
        "    lr_batch = lr_batch.to(device)\n",
        "\n",
        "    # Perform a forward pass\n",
        "    predictions = model(lr_batch)\n",
        "\n",
        "    print(f\"Sanity check successful!\")\n",
        "    print(f\"Running on device: {device}\")\n",
        "    print(f\"Model Input Shape (LR): {lr_batch.shape}\")\n",
        "    print(f\"Model Output Shape (Predictions): {predictions.shape}\")\n",
        "    print(f\"Target Shape (HR): {hr_batch.shape}\")\n",
        "\n",
        "# Compare output shape with the target High-Resolution shape\n",
        "assert predictions.shape == hr_batch.shape, \"Model output shape does not match target HR shape!\"\n",
        "print(\"Output shape matches target shape. Ready for training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgnXeFZE8QJQ",
        "outputId": "72fe37cd-983a-4c4a-9e2a-54c3147077c4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing a forward pass sanity check...\n",
            "Sanity check successful!\n",
            "Running on device: cuda\n",
            "Model Input Shape (LR): torch.Size([16, 3, 128, 128])\n",
            "Model Output Shape (Predictions): torch.Size([16, 3, 256, 256])\n",
            "Target Shape (HR): torch.Size([16, 3, 256, 256])\n",
            "Output shape matches target shape. Ready for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Configure and Launch the Trainer"
      ],
      "metadata": {
        "id": "csIBPYQpNALf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custum Trainer with Checkpoints"
      ],
      "metadata": {
        "id": "8NRbqBcmcRXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomResumableTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.optimizer = None\n",
        "        self.scheduler = None\n",
        "\n",
        "    def save_checkpoint(self, epoch, is_best=False):\n",
        "        \"\"\"Saves a complete training state checkpoint.\"\"\"\n",
        "        output_dir = self.args.output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Determine the filename\n",
        "        if is_best:\n",
        "            filename = 'best_model_checkpoint.pt'\n",
        "        else:\n",
        "            filename = 'latest_step_checkpoint.pt'\n",
        "\n",
        "        checkpoint_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        state = {\n",
        "            'epoch': epoch, 'best_metric': self.best_metric,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "        }\n",
        "        torch.save(state, checkpoint_path)\n",
        "        logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        \"\"\"Loads the most recent checkpoint available.\"\"\"\n",
        "        output_dir = self.args.output_dir\n",
        "        step_checkpoint_path = os.path.join(output_dir, 'latest_step_checkpoint.pt')\n",
        "        best_checkpoint_path = os.path.join(output_dir, 'best_model_checkpoint.pt')\n",
        "        start_epoch = 0\n",
        "\n",
        "        # Prioritize the very latest step-based checkpoint for resumption\n",
        "        checkpoint_path = None\n",
        "        if os.path.exists(step_checkpoint_path):\n",
        "            checkpoint_path = step_checkpoint_path\n",
        "        elif os.path.exists(best_checkpoint_path):\n",
        "            checkpoint_path = best_checkpoint_path\n",
        "\n",
        "        if checkpoint_path is None:\n",
        "            logger.warning(\"No checkpoint found. Starting from scratch.\")\n",
        "            return start_epoch\n",
        "\n",
        "        try:\n",
        "            state = torch.load(checkpoint_path, map_location=self.args.device)\n",
        "            self.model.load_state_dict(state['model_state_dict'])\n",
        "            if self.optimizer is None: self._create_optimizer_and_scheduler()\n",
        "            self.optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "            self.scheduler.load_state_dict(state['scheduler_state_dict'])\n",
        "            self.best_metric = state.get('best_metric', 0.0)\n",
        "            start_epoch = state['epoch'] # Resume from the same epoch\n",
        "            logger.info(f\"Successfully loaded checkpoint '{os.path.basename(checkpoint_path)}'. Resuming from epoch {start_epoch}.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load checkpoint: {e}. Starting from scratch.\")\n",
        "            start_epoch = 0\n",
        "        return start_epoch\n",
        "\n",
        "    def _create_optimizer_and_scheduler(self):\n",
        "        self.optimizer = Adam(self.model.parameters(), lr=self.args.learning_rate)\n",
        "        self.scheduler = lr_scheduler.StepLR(self.optimizer, step_size=999999, gamma=1.0) # Dummy scheduler, we control LR manually\n",
        "\n",
        "    def train(self, **kwargs):\n",
        "        \"\"\"Complete, resumable training loop with warm-up and step-based saving.\"\"\"\n",
        "        self._create_optimizer_and_scheduler()\n",
        "        start_epoch = self.load_checkpoint()\n",
        "        train_dataloader = self.get_train_dataloader()\n",
        "\n",
        "        global_step = start_epoch * len(train_dataloader)\n",
        "\n",
        "        for epoch in range(start_epoch, self.args.num_train_epochs):\n",
        "            self.model.train()\n",
        "            epoch_losses = AverageMeter()\n",
        "            with tqdm(total=len(train_dataloader), initial=global_step % len(train_dataloader)) as t:\n",
        "                t.set_description(f'epoch: {epoch}/{self.args.num_train_epochs - 1}')\n",
        "                for data in train_dataloader:\n",
        "                    # --- Learning Rate Scheduling (Warm-up + Decay) ---\n",
        "                    if global_step < self.args.warmup_steps:\n",
        "                        # Warm-up phase\n",
        "                        lr_scale = float(global_step) / float(self.args.warmup_steps)\n",
        "                        new_lr = self.args.learning_rate * lr_scale\n",
        "                    else:\n",
        "                        # Post-warm-up decay phase\n",
        "                        new_lr = self.args.learning_rate * (0.1 ** (epoch // int(self.args.num_train_epochs * 0.8)))\n",
        "\n",
        "                    for param_group in self.optimizer.param_groups:\n",
        "                        param_group['lr'] = new_lr\n",
        "\n",
        "                    inputs, labels = data\n",
        "                    inputs, labels = inputs.to(self.args.device), labels.to(self.args.device)\n",
        "                    preds = self.model(inputs)\n",
        "                    loss = torch.nn.L1Loss()(preds, labels)\n",
        "                    epoch_losses.update(loss.item(), len(inputs))\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                    global_step += 1\n",
        "                    t.set_postfix(loss=f'{epoch_losses.avg:.6f}', lr=f'{new_lr:.1e}')\n",
        "                    t.update(1)\n",
        "\n",
        "                    # --- Step-Based Checkpointing ---\n",
        "                    if global_step % self.args.save_steps == 0:\n",
        "                        self.save_checkpoint(epoch, is_best=False)\n",
        "\n",
        "            # --- Epoch-Based Evaluation and Best Model Saving ---\n",
        "            super().eval(epoch)\n",
        "            if self.best_epoch == epoch:\n",
        "                self.save_checkpoint(epoch, is_best=True)"
      ],
      "metadata": {
        "id": "qDbYjaEhKhW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer Config"
      ],
      "metadata": {
        "id": "5d8uiyuW4UWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainingArguments(TrainingArguments):\n",
        "    warmup_steps: int = 0\n",
        "    save_steps: int = 100 # Default to saving every 100 steps\n"
      ],
      "metadata": {
        "id": "qVJ9sh50Lpbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define DEFINITIVE Training Arguments based on a full-code review\n",
        "training_args = CustomTrainingArguments(\n",
        "    output_dir=FINETUNR_SAVE_DIR,\n",
        "\n",
        "    # --- Core parameters that are fully functional ---\n",
        "    num_train_epochs=15,          # Controls training length and the hardcoded LR decay\n",
        "    learning_rate=1e-4,           # Sets the initial learning rate\n",
        "    per_device_train_batch_size=24, # Controls training batch size\n",
        "\n",
        "    # --- Technical parameters that are functional ---\n",
        "    seed=42,                      # For reproducibility\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    dataloader_num_workers=2,\n",
        "    save_steps=100,      # Save a recovery checkpoint every 100 steps\n",
        "    warmup_steps=500     # Use a 500-step learning rate warm-up\n",
        ")"
      ],
      "metadata": {
        "id": "L3mw9OL7iOxs"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Training"
      ],
      "metadata": {
        "id": "NDzl-_Nz4g5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = CustomResumableTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "Du4n51ivdOjU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Training with full confidence in the underlying process.\n",
        "print(\"Starting model fine-tuning \")\n",
        "trainer.train()\n",
        "\n",
        "print(f\"\\nTraining complete. The best model was found at epoch {trainer.best_epoch} \"\n",
        "      f\"with a PSNR of {trainer.best_metric:.2f}.\")\n",
        "print(f\"The best model has been saved in: {FINETUNR_SAVE_DIR}\")"
      ],
      "metadata": {
        "id": "fKxQxP2f4Nsr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "2f72ac18adff47ff925a4f2850dc36fc",
            "fa9d44503708453b9482591030350704",
            "04449d91732f416dab9a9d0f2f89f5d5",
            "950106b1dcc148dc9586bbb1000f99be",
            "ebe0c8a19ad945679b7faa1702108d96",
            "60362261071f4b099396f41371389e65",
            "d563d0f4dd084432adfae11c7752ff57",
            "29eb2105ed754070ad2b1621b3430fb1",
            "839c28a44918423392a4d0000f526d97",
            "5f4e0421aa984f91a0d841ddc1451418",
            "5dd688f11e194fcdb9180aa6a3b6109c"
          ]
        },
        "outputId": "d7e1e986-3cb2-43dd-dd73-67f671ebcd5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:super_image.trainer:No training checkpoint found. Starting from scratch.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting model fine-tuning \n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f72ac18adff47ff925a4f2850dc36fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4416 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    }
  ]
}