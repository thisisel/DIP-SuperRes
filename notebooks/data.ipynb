{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2D-hZyrenUa"
      },
      "source": [
        "# Preparing the Enviorment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4OPbq8XHf8j"
      },
      "source": [
        "## Intsall Essential Packaeges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x0-tmeMHWDy",
        "outputId": "3d1fbc10-477f-4ca7-eede-c2c16a1c2cb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install tacoreader rasterio --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82QwEq9LDonk"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Fx67v5BDcrA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "from torchvision.transforms import v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1jJY4AFDsED"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import tacoreader\n",
        "from tacoreader import TortillaDataFrame\n",
        "import numpy as np\n",
        "import rasterio as rio\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from typing import List, Dict, Callable, Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVZD6TACHoF3"
      },
      "source": [
        "## Paths and Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWhUEwctHfOt",
        "outputId": "c8a44b7b-eca1-486f-e0d9-db5c3c904fbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGOmNWIBDlJA",
        "outputId": "20ea1caf-2034-48a9-f819-0fead81bac86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data will be saved to: /content/drive/MyDrive/datasets/sen2venus/TACO_raw_data\n",
            "Normalaized datest will be saved to: /content/drive/MyDrive/datasets/sen2venus/normalized_sets\n",
            "Train data will be saved to: /content/drive/MyDrive/datasets/sen2venus/normalized_sets/train\n",
            "Validation data will be saved to: /content/drive/MyDrive/datasets/sen2venus/normalized_sets/val\n",
            "Test data will be saved to: /content/drive/MyDrive/datasets/sen2venus/normalized_sets/test\n"
          ]
        }
      ],
      "source": [
        "ROOT_PATH = Path('/content/drive/MyDrive/datasets/sen2venus')\n",
        "\n",
        "TACO_RAW_DIR = ROOT_PATH / 'TACO_raw_data'\n",
        "os.makedirs(TACO_RAW_DIR, exist_ok=True)\n",
        "print(f\"Data will be saved to: {TACO_RAW_DIR}\")\n",
        "\n",
        "SELECTED_SUBSETS = [\n",
        "    \"SUDOUE-4\",\n",
        "    \"SUDOUE-5\",\n",
        "    \"SUDOUE-6\"\n",
        "]\n",
        "TACO_FILE_PATHS = [TACO_RAW_DIR / f\"{site_name}.taco\" for site_name in SELECTED_SUBSETS]\n",
        "\n",
        "\n",
        "NORMALIZED_SETS_DIR = ROOT_PATH / 'normalized_sets'\n",
        "os.makedirs(NORMALIZED_SETS_DIR, exist_ok=True)\n",
        "print(f\"Normalaized datest will be saved to: {NORMALIZED_SETS_DIR}\")\n",
        "\n",
        "TRAIN_SAVE_DIR = NORMALIZED_SETS_DIR / 'train'\n",
        "os.makedirs(TRAIN_SAVE_DIR, exist_ok=True)\n",
        "print(f\"Train data will be saved to: {TRAIN_SAVE_DIR}\")\n",
        "\n",
        "VAL_SAVE_DIR = NORMALIZED_SETS_DIR / 'val'\n",
        "os.makedirs(VAL_SAVE_DIR, exist_ok=True)\n",
        "print(f\"Validation data will be saved to: {VAL_SAVE_DIR}\")\n",
        "\n",
        "TEST_SAVE_DIR = NORMALIZED_SETS_DIR / 'test'\n",
        "os.makedirs(TEST_SAVE_DIR, exist_ok=True)\n",
        "print(f\"Test data will be saved to: {TEST_SAVE_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMZqIzV0ViYw"
      },
      "source": [
        "# Step 1: Download the Selected Regions (one time only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYKW6vjUVn-o"
      },
      "outputs": [],
      "source": [
        "SELECTED_SUBSETS = [\n",
        "    # \"SUDOUE-4\",\n",
        "    \"SUDOUE-5\",\n",
        "    \"SUDOUE-6\"\n",
        "]\n",
        "\n",
        "\n",
        "def get_taco_subset(site_name: str)->TortillaDataFrame:\n",
        "  dataset = tacoreader.load(\"tacofoundation:sen2venus\")\n",
        "\n",
        "  # The 'region' column holds the site name, e.g., 'SUDOUE-4'\n",
        "  site_df = dataset[dataset[\"region\"] == site_name]\n",
        "\n",
        "  if len(site_df) == 0:\n",
        "    raise ValueError(f'invalid site name {site_name}')\n",
        "\n",
        "  return site_df\n",
        "\n",
        "\n",
        "def compile_save_subset(output_filename: str| Path, site_df: TortillaDataFrame)->None:\n",
        "  print(f\"Compiling {len(site_df)} patches into {output_filename}...\")\n",
        "  tacoreader.compile(dataframe=site_df, output=output_filename, nworkers=8)\n",
        "\n",
        "\n",
        "def get_compile_subsets(site_names : List[str])->None:\n",
        "  for site_name in site_names:\n",
        "    print(f\"\\nProcessing site: {site_name}\")\n",
        "    site_df = get_taco_subset(site_name)\n",
        "    output_filename = TACO_RAW_DIR / f\"{site_name}.taco\"\n",
        "    compile_save_subset(output_filename, site_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQXWGfVCjJzx",
        "outputId": "6911f06c-4d4e-468c-aeb6-07770479d6d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing site: SUDOUE-5\n",
            "Compiling 2176 patches into /content/drive/MyDrive/datasets/sen2venus/TACO_raw_data/SUDOUE-5.taco...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rolling out a tortilla: 100%|██████████| 2176/2176 [02:20<00:00, 15.48file/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing site: SUDOUE-6\n",
            "Compiling 2435 patches into /content/drive/MyDrive/datasets/sen2venus/TACO_raw_data/SUDOUE-6.taco...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Grilling a tortilla 🫓: 100%|██████████| 2435/2435 [02:46<00:00, 14.62file/s]\n"
          ]
        }
      ],
      "source": [
        "get_compile_subsets(SELECTED_SUBSETS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FALnU7cgKhx"
      },
      "source": [
        "# Step 2: Define PyTorch Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEeypD-KgM7P"
      },
      "outputs": [],
      "source": [
        "class TACORGBDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset for reading compiled .taco files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        taco_file_paths: List[str | Path],\n",
        "        transform: Callable = None,\n",
        "        lr_band_combo: List[int] = [1, 2, 3], # Blue, Green, Red of Sentinel-2\n",
        "        hr_band_combo: List[int] = [1, 2, 3], # Blue, Green, Red of venus\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            taco_file_paths (List[str]): A list of file paths to the .taco files.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.taco_files = [str(p) for p in taco_file_paths]\n",
        "        self.transform = transform\n",
        "        self.lr_band_combo = lr_band_combo\n",
        "        self.hr_band_combo = hr_band_combo\n",
        "\n",
        "        # Load and concatenate all specified .taco files into a single dataframe\n",
        "        print(f\"Loading data from: {self.taco_files}\")\n",
        "        # Convert PosixPath objects to strings\n",
        "        list_of_dfs = [tacoreader.load(p) for p in self.taco_files]\n",
        "        self.dataframe = pd.concat(list_of_dfs, ignore_index=True)\n",
        "        print(f\"Successfully loaded a total of {len(self.dataframe)} samples.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Get the data for one row from the dataframe\n",
        "        row = self.dataframe.read(idx)\n",
        "\n",
        "        # Retrieve the data\n",
        "        lr, hr = row.read(0), row.read(1)\n",
        "        with rio.open(lr) as src_lr, rio.open(hr) as src_hr:\n",
        "            lr_image_np = src_lr.read(self.lr_band_combo) \n",
        "            hr_image_np = src_hr.read(self.hr_band_combo)  \n",
        "\n",
        "        # Convert directly to PyTorch Tensors. This preserves the uint16 data.\n",
        "        lr_tensor = torch.from_numpy(lr_image_np)\n",
        "        hr_tensor = torch.from_numpy(hr_image_np)\n",
        "\n",
        "        lr_tensor = lr_tensor.float()\n",
        "        hr_tensor = hr_tensor.float()\n",
        "\n",
        "        sample = {\"lr\": lr_tensor, \"hr\": hr_tensor}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "class NormalizedTacoDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Wrapper Datasetclass for applying normalization\n",
        "    on Tran/Test/Validation subsets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Gets the raw tensor from the subset\n",
        "        sample = self.subset[index]\n",
        "        # Applies the final normalization transform\n",
        "        return self.transform(sample)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "\n",
        "class PreNormalizedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    An efficient Dataset that reads pre-processed, sharded tensor files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, shard_dir: Union[str, Path]):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            shard_dir (Union[str, Path]): The directory containing the shard files.\n",
        "        \"\"\"\n",
        "        # Immediately convert the input to a Path object to standardize its type.\n",
        "        self.shard_dir = Path(shard_dir)\n",
        "\n",
        "        # Use the object-oriented .glob() method to find all .pt files.\n",
        "        # It returns a generator, so we convert to a list before sorting.\n",
        "        self.shard_paths: List[Path] = sorted(self.shard_dir.glob(\"*.pt\"))\n",
        "\n",
        "        if not self.shard_paths:\n",
        "            raise ValueError(f\"No shard files ('*.pt') found in {self.shard_dir}\")\n",
        "\n",
        "        # The rest of the logic remains the same, but is more robust\n",
        "        # because torch.load() works seamlessly with Path objects.\n",
        "\n",
        "        # Load the first shard to determine the shard_size\n",
        "        first_shard = torch.load(self.shard_paths[0])\n",
        "        self.shard_size = len(first_shard)\n",
        "\n",
        "        # Load the last shard to calculate the total length accurately\n",
        "        last_shard = torch.load(self.shard_paths[-1])\n",
        "        self.length = (len(self.shard_paths) - 1) * self.shard_size + len(last_shard)\n",
        "\n",
        "        # A simple cache to hold the most recently used shard\n",
        "        self._cache = {}\n",
        "        # A variable to keep track of which shard is currently in the cache\n",
        "        self._cached_shard_index = -1\n",
        "\n",
        "        print(\n",
        "            f\"Initialized dataset from {self.shard_dir} with {self.length} samples across {len(self.shard_paths)} shards.\"\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        shard_index = idx // self.shard_size\n",
        "        index_in_shard = idx % self.shard_size\n",
        "\n",
        "        # Check if the required shard is currently cached\n",
        "        if shard_index != self._cached_shard_index:\n",
        "            # If not, load the correct shard from disk and update the cache\n",
        "            shard_path = self.shard_paths[shard_index]\n",
        "            self._cache = torch.load(shard_path)\n",
        "            self._cached_shard_index = shard_index\n",
        "\n",
        "        # coupled with TACORGBDataset dataset class\n",
        "        # each item in the shard is a squeezed dictionary with keys lr and hr\n",
        "        squeezed_sample = self._cache[index_in_shard]\n",
        "        return squeezed_sample[\"lr\"], squeezed_sample[\"hr\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcHdN6sdh3Wn",
        "outputId": "0a6e15df-1dc1-44f7-c5bb-24f53b3a0aa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from: ['/content/drive/MyDrive/datasets/sen2venus/TACO_raw_data/SUDOUE-4.taco', '/content/drive/MyDrive/datasets/sen2venus/TACO_raw_data/SUDOUE-5.taco', '/content/drive/MyDrive/datasets/sen2venus/TACO_raw_data/SUDOUE-6.taco']\n",
            "Successfully loaded a total of 5546 samples.\n"
          ]
        }
      ],
      "source": [
        "full_dataset = TACORGBDataset(taco_file_paths=TACO_FILE_PATHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoViYUoJGNSa",
        "outputId": "d84bb707-2572-4f15-d647-318e85f5ecea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR image tensor shape: torch.Size([3, 128, 128])\n",
            "HR image tensor shape: torch.Size([3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "first_item = full_dataset[101]\n",
        "print(f\"LR image tensor shape: {first_item['lr'].shape}\")\n",
        "print(f\"HR image tensor shape: {first_item['hr'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxqtGUoCgccQ"
      },
      "source": [
        "# Step 3: Split the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goLtCxx7ge7U",
        "outputId": "d6b32dce-e76a-4b29-805d-bb09b3a40228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 4436, Validation samples: 554, Test samples: 556\n"
          ]
        }
      ],
      "source": [
        "# Define split sizes\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = int(0.1 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "# Perform the split\n",
        "train_subset, val_subset, test_subset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "print(f\"Train samples: {len(train_subset)}, Validation samples: {len(val_subset)}, Test samples: {len(test_subset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjignJ5ApF66"
      },
      "source": [
        "# Step 4: Confirm the Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZPh6dDZorOt"
      },
      "source": [
        "> Several preprocessing steps have already been applied to the dataset by `TACO foundation`:\n",
        "\n",
        "- **Handling of negative pixel values:** Some Sentinel-2 images contained negative pixel values, which have been masked to **0**.\n",
        "- **No-data values:** All no-data values are now set to **65525** for consistency.\n",
        "- **Data type conversion:** The original dataset was stored in **int16**, and it has been converted to **uint16** for storage and processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvkjjBifeOpX"
      },
      "source": [
        "# Step 5: Statistical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBcfgL_eKY_k"
      },
      "outputs": [],
      "source": [
        "def get_subset_stats(loader: DataLoader):\n",
        "    \"\"\"\n",
        "    Calculates statistics by iterating through a DataLoader, processing one batch at a time.\n",
        "    This is significantly more efficient than loading all samples into memory first.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize trackers for the statistics we need\n",
        "    # For min/max, we can track the running value.\n",
        "    # For mean/std, the correct way to aggregate across batches is to track sums.\n",
        "\n",
        "    # Initialize min/max trackers for 3 channels (R, G, B)\n",
        "    lr_max = torch.tensor([-float('inf')] * 3)\n",
        "    hr_max = torch.tensor([-float('inf')] * 3)\n",
        "    lr_min = torch.tensor([float('inf')] * 3)\n",
        "    hr_min = torch.tensor([float('inf')] * 3)\n",
        "\n",
        "    # Trackers for a numerically stable standard deviation calculation\n",
        "    lr_sum = torch.zeros(3)\n",
        "    hr_sum = torch.zeros(3)\n",
        "    lr_sum_sq = torch.zeros(3)\n",
        "    hr_sum_sq = torch.zeros(3)\n",
        "    pixel_count = 0\n",
        "\n",
        "    # Iterate through the batches provided by the DataLoader\n",
        "    for batch in tqdm(loader, desc=\"Calculating Stats\"):\n",
        "        lr_batch = batch['lr'] # Shape: (B, C, H, W) e.g., (64, 3, 128, 128)\n",
        "        hr_batch = batch['hr']\n",
        "\n",
        "        # --- Min/Max Calculation (Batch-wise) ---\n",
        "        # Find the max value along all dims except the channel dim\n",
        "        current_lr_max = torch.amax(lr_batch, dim=(0, 2, 3))\n",
        "        current_hr_max = torch.amax(hr_batch, dim=(0, 2, 3))\n",
        "        # Update our running global max\n",
        "        lr_max = torch.maximum(lr_max, current_lr_max)\n",
        "        hr_max = torch.maximum(hr_max, current_hr_max)\n",
        "\n",
        "        # Do the same for min\n",
        "        current_lr_min = torch.amin(lr_batch, dim=(0, 2, 3))\n",
        "        current_hr_min = torch.amin(hr_batch, dim=(0, 2, 3))\n",
        "        lr_min = torch.minimum(lr_min, current_lr_min)\n",
        "        hr_min = torch.minimum(hr_min, current_hr_min)\n",
        "\n",
        "        # --- Mean/Std Calculation (Batch-wise) ---\n",
        "        # Sum up all values per channel for this batch\n",
        "        lr_sum += torch.sum(lr_batch, dim=(0, 2, 3))\n",
        "        hr_sum += torch.sum(hr_batch, dim=(0, 2, 3))\n",
        "        # Sum up the squares of all values per channel\n",
        "        lr_sum_sq += torch.sum(lr_batch**2, dim=(0, 2, 3))\n",
        "        hr_sum_sq += torch.sum(hr_batch**2, dim=(0, 2, 3))\n",
        "\n",
        "        # Count the total number of pixels per channel\n",
        "        # Batch size * height * width\n",
        "        pixel_count += lr_batch.shape[0] * lr_batch.shape[2] * lr_batch.shape[3]\n",
        "\n",
        "    # Finalize the mean and standard deviation calculations\n",
        "    mean_lr = lr_sum / pixel_count\n",
        "    mean_hr = hr_sum / pixel_count\n",
        "\n",
        "    # std_lr = torch.sqrt((lr_sum_sq / pixel_count) - mean_lr**2)\n",
        "    # std_hr = torch.sqrt((hr_sum_sq / pixel_count) - mean_hr**2)\n",
        "\n",
        "    # Inside get_stats_efficiently, before the return statement:\n",
        "    variance_lr = (lr_sum_sq / pixel_count) - mean_lr**2\n",
        "    variance_hr = (hr_sum_sq / pixel_count) - mean_hr**2\n",
        "\n",
        "    # Clamp the variance at 0 to prevent negative values from causing nans in the sqrt.\n",
        "    std_lr = torch.sqrt(torch.clamp(variance_lr, min=0))\n",
        "    std_hr = torch.sqrt(torch.clamp(variance_hr, min=0))\n",
        "\n",
        "    # Compile results into a dictionary\n",
        "    stats = {\n",
        "        'lr_max_per_channel': lr_max,\n",
        "        'hr_max_per_channel': hr_max,\n",
        "        'lr_min_per_channel': lr_min,\n",
        "        'hr_min_per_channel': hr_min,\n",
        "        'lr_mean_per_channel': mean_lr,\n",
        "        'hr_mean_per_channel': mean_hr,\n",
        "        'lr_std_per_channel': std_lr,\n",
        "        'hr_std_per_channel': std_hr,\n",
        "    }\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glZ8qbU3J-cA"
      },
      "source": [
        "## Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb7VpYyeJ2dS"
      },
      "outputs": [],
      "source": [
        "num_workers = 2\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQHCbQ44oF0L"
      },
      "source": [
        "### Train Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF8R5LvNd-Wk",
        "outputId": "cfe614f4-858c-47b9-d506-95d67a4bea99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Stats: 100%|██████████| 70/70 [08:02<00:00,  6.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculated Statistics ---\n",
            "lr_max_per_channel: [8683.0, 9235.0, 11554.0]\n",
            "hr_max_per_channel: [11557.0, 11554.0, 12518.0]\n",
            "lr_min_per_channel: [0.0, 0.0, 0.0]\n",
            "hr_min_per_channel: [0.0, 0.0, 0.0]\n",
            "lr_mean_per_channel: [335.5336608886719, 572.8374633789062, 544.6136474609375]\n",
            "hr_mean_per_channel: [1360.9925537109375, 2311.95947265625, 2198.640380859375]\n",
            "lr_std_per_channel: [218.60350036621094, 264.00384521484375, 376.52978515625]\n",
            "hr_std_per_channel: [nan, nan, nan]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_stats_loader = DataLoader(\n",
        "    train_subset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    shuffle=False  # No need to shuffle for statistical analysis\n",
        ")\n",
        "\n",
        "train_stats = get_subset_stats(train_stats_loader)\n",
        "\n",
        "print(\"\\n--- Calculated Statistics ---\")\n",
        "for key, value in train_stats.items():\n",
        "    print(f\"{key}: {value.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrhTF103oNBd"
      },
      "source": [
        "### Validation Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE_RhK9G1jWy",
        "outputId": "f3efc587-7ff2-4ed9-ab63-ffc4991e7070"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Stats: 100%|██████████| 9/9 [00:30<00:00,  3.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Validation Calculated Statistics ---\n",
            "lr_max_per_channel: [10978.0, 8375.0, 8591.0]\n",
            "hr_max_per_channel: [8389.0, 8882.0, 9001.0]\n",
            "lr_min_per_channel: [0.0, 0.0, 0.0]\n",
            "hr_min_per_channel: [0.0, 0.0, 0.0]\n",
            "lr_mean_per_channel: [341.903076171875, 577.227294921875, 552.5531005859375]\n",
            "hr_mean_per_channel: [1379.35986328125, 2324.22900390625, 2228.824951171875]\n",
            "lr_std_per_channel: [224.30386352539062, 269.01324462890625, 382.1163024902344]\n",
            "hr_std_per_channel: [nan, nan, nan]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "val_stats_loader = DataLoader(\n",
        "    val_subset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    shuffle=False  # No need to shuffle for statistical analysis\n",
        ")\n",
        "\n",
        "val_stats = get_subset_stats(val_stats_loader)\n",
        "\n",
        "print(\"\\n--- Validation Calculated Statistics ---\")\n",
        "for key, value in val_stats.items():\n",
        "    print(f\"{key}: {value.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqvankDYoQEc"
      },
      "source": [
        "### Test Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZinKcw_plXO",
        "outputId": "e60b7f97-880f-4f21-911a-7ee78a2669f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating Stats: 100%|██████████| 9/9 [00:18<00:00,  2.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Test Calculated Statistics ---\n",
            "lr_max_per_channel: [7727.0, 8081.0, 12752.0]\n",
            "hr_max_per_channel: [8000.0, 8175.0, 8194.0]\n",
            "lr_min_per_channel: [0.0, 0.0, 0.0]\n",
            "hr_min_per_channel: [0.0, 0.0, 0.0]\n",
            "lr_mean_per_channel: [335.8233337402344, 575.5809326171875, 545.2900390625]\n",
            "hr_mean_per_channel: [1350.3729248046875, 2309.218994140625, 2190.69384765625]\n",
            "lr_std_per_channel: [218.07757568359375, 263.7121887207031, 376.3913269042969]\n",
            "hr_std_per_channel: [nan, nan, nan]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test_stats_loader = DataLoader(\n",
        "    test_subset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    shuffle=False  # No need to shuffle for statistical analysis\n",
        ")\n",
        "\n",
        "test_stats = get_subset_stats(test_stats_loader)\n",
        "\n",
        "print(\"\\n--- Test Calculated Statistics ---\")\n",
        "for key, value in test_stats.items():\n",
        "    print(f\"{key}: {value.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MRHCeSre0EJ"
      },
      "source": [
        "# Step 6: Subset Normalization (MinMax Scaling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrRed2Ps6n5c"
      },
      "outputs": [],
      "source": [
        "def normalize_tensor_sample(sample: Dict[str, torch.Tensor],\n",
        "                               lr_max_per_channel: torch.Tensor,\n",
        "                               hr_max_per_channel: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Normalizes a sample dictionary of multi-channel float tensors\n",
        "    using per-channel maximum values.\n",
        "\n",
        "    Args:\n",
        "        sample (Dict[str, torch.Tensor]): The input sample, e.g., {'lr': tensor, 'hr': tensor}.\n",
        "        lr_max_per_channel (torch.Tensor): A 1D tensor of shape (3,) containing the max values for the R, G, B channels of the LR image.\n",
        "        hr_max_per_channel (torch.Tensor): A 1D tensor of shape (3,) for the HR image.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, torch.Tensor]: The normalized sample.\n",
        "    \"\"\"\n",
        "    # Reshape the max-value tensors from (3,) to (3, 1, 1) for broadcasting.\n",
        "    # This tells PyTorch to align the channel dimension and \"stretch\" the other two.\n",
        "    lr_max_reshaped = lr_max_per_channel.view(3, 1, 1)\n",
        "    hr_max_reshaped = hr_max_per_channel.view(3, 1, 1)\n",
        "\n",
        "    # Perform the per-channel division.\n",
        "    # PyTorch will broadcast the (3, 1, 1) tensor across the (3, H, W) image tensor.\n",
        "    # This divides each channel by its specific max value efficiently.\n",
        "    lr = sample['lr'] / lr_max_reshaped\n",
        "    hr = sample['hr'] / hr_max_reshaped\n",
        "\n",
        "    # Clamp values to ensure they are strictly within [0,1]\n",
        "    return {'lr': torch.clamp(lr, 0.0, 1.0), 'hr': torch.clamp(hr, 0.0, 1.0)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGBnYbV4BjyI"
      },
      "outputs": [],
      "source": [
        "lr_max_per_channel = [8683.0, 9235.0, 11554.0]\n",
        "hr_max_per_channel = [11557.0, 11554.0, 12518.0]\n",
        "\n",
        "train_lr_maxes = torch.tensor(lr_max_per_channel, dtype=torch.float32)\n",
        "train_hr_maxes = torch.tensor(hr_max_per_channel, dtype=torch.float32)\n",
        "\n",
        "# Using a lambda to pass  calculated tensors to the normalization function\n",
        "final_transform = lambda sample: normalize_tensor_sample(sample,\n",
        "                                                            lr_max_per_channel=train_lr_maxes,\n",
        "                                                            hr_max_per_channel=train_hr_maxes)\n",
        "\n",
        "\n",
        "train_dataset = NormalizedTacoDataset(train_subset, final_transform)\n",
        "val_dataset = NormalizedTacoDataset(val_subset, final_transform)\n",
        "test_dataset = NormalizedTacoDataset(test_subset, final_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKFLFKB2CD0f",
        "outputId": "1eee54c0-78e9-4db7-b01e-8501682cf9fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Max value in final normalized LR Red channel: 0.0955\n",
            "Max value in final normalized LR Green channel: 0.1449\n",
            "Max value in final normalized LR Blue channel: 0.1431\n"
          ]
        }
      ],
      "source": [
        "sample_to_check = train_dataset[0]\n",
        "lr_check = sample_to_check['lr']\n",
        "print(f\"\\nMax value in final normalized LR Red channel: {lr_check[0].max():.4f}\")   # Should be <= 1.0\n",
        "print(f\"Max value in final normalized LR Green channel: {lr_check[1].max():.4f}\") # Should be <= 1.0\n",
        "print(f\"Max value in final normalized LR Blue channel: {lr_check[2].max():.4f}\")  # Should be <= 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee7hQ_NVDXsB"
      },
      "source": [
        "# Step 7: Save Normalized Subsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAImYfxNDbTk"
      },
      "outputs": [],
      "source": [
        "def save_processed_dataset(dataset: Dataset, save_dir: str, shard_size: int = 1000):\n",
        "    \"\"\"\n",
        "    Iterates through a dataset and saves the processed tensors in sharded .pt files.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): The dataset to save (e.g.,train_dataset).\n",
        "        save_dir (str): The directory  to save the shards.\n",
        "        shard_size (int): The number of samples to store in each shard file.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Use a DataLoader for efficient reading from the source dataset\n",
        "    loader = DataLoader(dataset, batch_size=1, num_workers=2)\n",
        "\n",
        "    shard = []\n",
        "    shard_index = 0\n",
        "\n",
        "    for i, sample in enumerate(tqdm(loader, desc=f\"Saving to {os.path.basename(save_dir)}\")):\n",
        "        # The loader adds an extra batch dimension, so we squeeze it out.\n",
        "        # We also move tensors to CPU to ensure they can be saved from any device.\n",
        "        squeezed_sample = {key: val.squeeze(0).cpu() for key, val in sample.items()}\n",
        "        shard.append(squeezed_sample)\n",
        "\n",
        "        # When the shard is full, or it's the last sample, save it\n",
        "        if len(shard) == shard_size or i == len(loader) - 1:\n",
        "            shard_path = os.path.join(save_dir, f\"shard_{shard_index}.pt\")\n",
        "            torch.save(shard, shard_path)\n",
        "\n",
        "            # Reset for the next shard\n",
        "            shard = []\n",
        "            shard_index += 1\n",
        "\n",
        "    print(f\"\\nSuccessfully saved {i+1} samples in {shard_index} shards to {save_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jpMkA_tFSLt",
        "outputId": "0172c7bc-e148-4e2e-dc4b-02ab62fc4bf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Saving Training Set ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving to train: 100%|██████████| 4436/4436 [23:03<00:00,  3.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully saved 4436 samples in 5 shards to /content/drive/MyDrive/datasets/sen2venus/normalized_sets/train\n",
            "\n",
            "--- Saving Validation Set ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving to val: 100%|██████████| 554/554 [01:16<00:00,  7.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully saved 554 samples in 1 shards to /content/drive/MyDrive/datasets/sen2venus/normalized_sets/val\n",
            "\n",
            "--- Saving Test Set ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving to test: 100%|██████████| 556/556 [00:59<00:00,  9.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully saved 556 samples in 1 shards to /content/drive/MyDrive/datasets/sen2venus/normalized_sets/test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Saving Training Set ---\")\n",
        "save_processed_dataset(train_dataset, TRAIN_SAVE_DIR)\n",
        "\n",
        "print(\"\\n--- Saving Validation Set ---\")\n",
        "save_processed_dataset(val_dataset, VAL_SAVE_DIR)\n",
        "\n",
        "print(\"\\n--- Saving Test Set ---\")\n",
        "save_processed_dataset(test_dataset, TEST_SAVE_DIR)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DIP-SuperRes (3.12.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
